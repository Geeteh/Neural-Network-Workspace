# -*- coding: utf-8 -*-
"""PyTorch_Practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JF8aXK3HNCK5YFa8HpMo52E-rMAtPIsl
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.optim import Adam

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparams
input_size = 784
hidden_size = 100
num_classes = 10
learning_rate = 0.001
batch_size = 100
epochs = 2

#NN
class NN(nn.Module):
  def __init__(self, input_size, hidden_size, num_classes):
    super(NN, self).__init__()
    self.l1 = nn.Linear(input_size, hidden_size)
    self.relu = nn.ReLU()
    self.l2 = nn.Linear(hidden_size, num_classes)

  def forward(self, x):
    out = self.l1(x)
    out = self.relu(out)
    out = self.l2(out)
    return out

# Datasets
train_dataset = datasets.MNIST(root='dataset/', train = True, transform = transforms.ToTensor(), download = True)
train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)
test_dataset = datasets.MNIST(root='dataset/', train = False, transform = transforms.ToTensor(), download = True)
test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)

# Init NN model
model = NN(input_size, hidden_size, num_classes).to(device)

# Loss and optimize
criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=learning_rate)

# Train
for epoch in range(epochs):
  for batch_index, (data, labels) in enumerate(train_loader):
    data = data.reshape(-1,784).to(device)
    labels = labels.to(device)

    # Forward
    outputs = model(data)
    loss = criterion(outputs, labels)

    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (batch_index + 1) % batch_size == 0:
      print(f'epoch {epoch + 1} / {epochs}, step {batch_index + 1} / {len(train_loader)}, loss = {loss.item()}')

# Test
with torch.no_grad():
    n_correct = 0
    n_samples = 0
    for data, labels in test_loader:
      data = data.reshape(-1, 784).to(device)
      labels = labels.to(device)
      outputs = model(data)
      _, predictions = torch.max(outputs, 1)
      n_samples += labels.shape[0]
      n_correct += (predictions == labels).sum().item()
    
    accuracy = 100*n_correct / n_samples
    print("accuracy =",accuracy)